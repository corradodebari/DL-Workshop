{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Network used on a new set of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTION: Sample based on \"Fine-tune InceptionV3 on a new set of classes\" from https://keras.io/applications/  \n",
    "\n",
    "\n",
    "**Pre-requisites**:\n",
    "on an OCI GPU instance NVIDIA NGC:  \n",
    "nvidia-docker run -it -d -p 8888:8888 -p 6006:6006 -v /home/ubuntu:/shared nvcr.io/nvidia/tensorflow:18.04-py2\n",
    "\n",
    "make two directory:  ''**data/training**' and '**data/test**'\n",
    "to reference into:  \n",
    "\n",
    "train_data_dir =  '**data/training**' #contains classes to train  \n",
    "validation_data_dir = '**data/test**' #contains classes for validation  \n",
    "\n",
    "Upload in training directory the images, creating a subfolder directory for each class  \n",
    "Upload in test directory images for validation, creating a subfolder  directory for each class  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras\n",
    "! pip install pillow\n",
    "! pip install Flask\n",
    "! pip install Flask-Uploads\n",
    "! pip install Flask-Uploads --upgrade\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Tensorflow release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input\n",
    "import PIL\n",
    "import os\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "train_data_dir =  'cars-dataset/training' #contains classes to train\n",
    "validation_data_dir = 'cars-dataset/validation' #contains classes for validation\n",
    "test_data_dir = 'cars-dataset/test'  #contains classes for testing\n",
    "\n",
    "nb_epoch = 7\n",
    "model_name = \"inception_retrained_model7\"\n",
    "nb_batchsize = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training & Validation Data load and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Training and Validation\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "print (\"\\nTraining directory:\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_batchsize,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "label_map = (train_generator.class_indices)\n",
    "\n",
    "print (\"\\nLabel map in training directory:\")\n",
    "print (label_map)\n",
    "print (\"\\nValidation directory:\")\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_batchsize,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_data_dir)])\n",
    "nb_validation_samples = sum([len(files) for r, d, files in os.walk(validation_data_dir)])\n",
    "nb_classes  = sum([len(d) for r, d, files in os.walk(validation_data_dir)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step: go to Load model re-trained cell to skip training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#print(device_lib.list_local_devices())\n",
    "print(\"Get available GPUs\")\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes =  len(label_map)\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "print(\"base_model\")\n",
    "#for i, layer in enumerate(model.layers):\n",
    "#    print(i, layer.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(model_name+\".h5\"):\n",
    "    model = load_model(model_name+\".h5\")\n",
    "    print(\"re-training pre-trained model\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 300 layers and unfreeze the rest:\n",
    "#for layer in model.layers[:300]:\n",
    "for layer in model.layers[:311]:\n",
    "    layer.trainable = False\n",
    "#for layer in model.layers[300:]:\n",
    "for layer in model.layers[312:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "# model.fit_generator(...)\n",
    "# fine-tune the model\n",
    "if (nb_batchsize>nb_validation_samples):\n",
    "    nb_validation_steps = 1\n",
    "else:\n",
    "    nb_validation_steps =int(nb_validation_samples/nb_batchsize)\n",
    "\n",
    "if (nb_batchsize>nb_train_samples):\n",
    "    nb_train_samples=1\n",
    "else:\n",
    "    nb_train_steps =int(nb_train_samples/nb_batchsize)\n",
    "\n",
    "print (nb_train_steps)\n",
    "print (nb_validation_steps)\n",
    "\n",
    "\n",
    "history=model.fit_generator(\n",
    "        train_generator,\n",
    "        #samples_per_epoch=nb_train_samples,\n",
    "        epochs=nb_epoch,\n",
    "        steps_per_epoch=nb_train_samples,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_steps,\n",
    "        verbose=1,\n",
    "        shuffle=True)\n",
    "        #nb_val_samples=nb_validation_samples,\n",
    "        #steps_per_epoch=int(nb_train_samples/nb_batchsize)\n",
    "\n",
    "\n",
    "model.save(model_name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "\n",
    "epochs = range(1, nb_epoch + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')          \n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')     \n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()                                    \n",
    "\n",
    "plt.plot(epochs,acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predictions\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model re-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model(model_name+\".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def inference(imagepath):\n",
    "    img_path = imagepath\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    preds = model.predict(x)\n",
    "    \n",
    "    # Decode results\n",
    "    inv_map = {v: k for k, v in label_map.iteritems()}\n",
    "    i=0\n",
    "    a = []\n",
    "    for prob in preds[0]:\n",
    "        a.append([i,int(prob*100)])\n",
    "        i+=1\n",
    "\n",
    "    sorted= numpy.argsort(a,axis=0)\n",
    "    final =[]\n",
    "    for elem in sorted:\n",
    "        final.append([inv_map[elem[1]],int(preds[0][elem[1]]*100)])\n",
    "    inference= numpy.flip(final,axis=0)\n",
    "    \n",
    "    partJson= '{ \\\"name\\\":\\\"'+imagepath+'\\\",  \\\"results\\\": [ '\n",
    "  \n",
    "    for i in range(0,nb_classes):\n",
    "        partJson+='{ \\\"class' +str(i+1) + '\\\":\\\"' + str(inference[i][0]) + '\\\", \\\"value\\\":\\\"' + str(inference[i][1]) + '\\\" },' \n",
    "    json = partJson[:-1]+ ']  }'\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on local image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test on a local image:\")\n",
    "result = inference(test_data_dir+'/door.jpg')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST Server for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, redirect, url_for,Response\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "UPLOAD_FOLDER = os.getcwd()+'/img'\n",
    "ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'])\n",
    "\n",
    "#app = Flask(__name__)\n",
    "app = Flask(os.getcwd())\n",
    "\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "def root_dir():  # pragma: no cover\n",
    "    #print('TEST')\n",
    "    #return os.path.abspath(os.path.dirname(__file__))\n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def get_file(filename):  # pragma: no cover\n",
    "    try:\n",
    "\n",
    "        src = os.path.join(root_dir(), filename)\n",
    "        # Figure out how flask returns static files\n",
    "        # Tried:\n",
    "        # - render_template\n",
    "        # - send_file\n",
    "        # This should not be so non-obvious\n",
    "        return open(src).read()\n",
    "    except IOError as exc:\n",
    "        return str(\"NOT_FOUND\")\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        print(root_dir())\n",
    "        # check if the post request has the file part\n",
    "        if 'file' not in request.files:\n",
    "            flash('No file part')\n",
    "            return redirect(request.url)\n",
    "        file = request.files['file']\n",
    "        # if user does not select file, browser also\n",
    "        # submit a empty part without filename\n",
    "        if file.filename == '':\n",
    "            flash('No selected file')\n",
    "            return redirect(request.url)\n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            #-----------------------------------------------------------------------------\n",
    "            # INFERENCE\n",
    "            result = inference(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            \n",
    "            #-----------------------------------------------------------------------------\n",
    "            \n",
    "            #file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            #open(os.path.join(app.config['UPLOAD_FOLDER'], filename+'.ck'),'w').close()\n",
    "            #return redirect(url_for('upload_file',filename=filename))\n",
    "            return result\n",
    "\n",
    "    return '''\n",
    "    <!doctype html>\n",
    "    <title>Upload new File</title>\n",
    "    <h1>Upload new File</h1>\n",
    "    <form method=post enctype=multipart/form-data>\n",
    "      <p><input type=file name=file accept=\"image/*\">\n",
    "         <input type=submit value=Upload>\n",
    "    </form>\n",
    "    '''\n",
    "\n",
    "@app.route('/inference', methods=['GET'])\n",
    "def metrics():  # pragma: no cover\n",
    "    fileResponse=request.args.get('filename', '')\n",
    "    content = get_file(UPLOAD_FOLDER+\"/\"+fileResponse+\".json\")\n",
    "    print(UPLOAD_FOLDER+\"/\"+fileResponse+\".json\")\n",
    "    return Response(content, mimetype=\"application/json\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(root_dir())\n",
    "    #app.run(debug=True)\n",
    "    app.run(host='0.0.0.0',port=6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
